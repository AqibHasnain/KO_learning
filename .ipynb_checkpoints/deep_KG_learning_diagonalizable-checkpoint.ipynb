{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hl_sizes):\n",
    "        super(Net, self).__init__()\n",
    "        current_dim = input_dim\n",
    "        self.linears = nn.ModuleList()\n",
    "        for hl_dim in hl_sizes:\n",
    "            self.linears.append(nn.Linear(current_dim, hl_dim))\n",
    "            current_dim = hl_dim\n",
    "        self.L = nn.Parameter(torch.rand(output_dim,requires_grad=True)) # vector of eigenvalues\n",
    "        self.V = nn.Parameter(torch.rand(output_dim,output_dim,requires_grad=True)) # matrix of eigenvalues\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_vecs = x\n",
    "        for layer in self.linears:\n",
    "            x = F.relu(layer(x))\n",
    "        x = torch.cat((torch.Tensor(np.ones((x.shape[0],1))),input_vecs,x),dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the state: 2\n",
      "Number of trajectories: 100\n",
      "Number of total snapshots: 8000\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.path.dirname('deep_KO_learning.py') # getting relative path\n",
    "trained_models_path = os.path.join(script_dir, 'trained_models') # which relative path do you want to see\n",
    "data_path = os.path.join(script_dir,'data/')\n",
    "\n",
    "file_dir = 'toggle_switch_data_normed.p'\n",
    "\n",
    "def get_snapshot_matrices(X,nT,nTraj): \n",
    "        '''This function assumes the global snapshot matrix is constructed with trajectories \n",
    "            sequentially placed in the columns'''\n",
    "        prevInds = [x for x in range(0,nT-1)]\n",
    "        forInds = [x for x in range(1,nT)]\n",
    "        for i in range(0,nTraj-1):\n",
    "            if i == 0:\n",
    "                more_prevInds = [x + nT for x in prevInds]\n",
    "                more_forInds = [x + nT for x in forInds]\n",
    "            else: \n",
    "                more_prevInds = [x + nT for x in more_prevInds]\n",
    "                more_forInds = [x + nT for x in more_forInds]\n",
    "            prevInds = prevInds + more_prevInds\n",
    "            forInds = forInds + more_forInds\n",
    "        Xp = X[:,prevInds]\n",
    "        Xf = X[:,forInds]\n",
    "        return Xp,Xf\n",
    "    \n",
    "X,nT,nTraj,dt_list = pickle.load(open(data_path+file_dir,'rb'))\n",
    "Xp,Xf = get_snapshot_matrices(X,nT,nTraj)\n",
    "trainXp = torch.Tensor(Xp.T)\n",
    "trainXf = torch.Tensor(Xf.T)\n",
    "testX = torch.Tensor(X.T)\n",
    "\n",
    "numDatapoints = nT*nTraj # number of total snapshots\n",
    "\n",
    "print('Dimension of the state: ' + str(trainXp.shape[1]));\n",
    "print('Number of trajectories: ' + str(nTraj));\n",
    "print('Number of total snapshots: ' + str(nT*nTraj));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "    (1): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (4): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (5): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (6): Linear(in_features=6, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUTS = trainXp.shape[1] # dimension of input\n",
    "NUM_HL = 6 # number of hidden layers (excludes the input layer)\n",
    "NODES_HL = 6 # number of nodes per hidden layer (number of learned observables)\n",
    "HL_SIZES = [NODES_HL for i in range(0,NUM_HL+1)] \n",
    "NUM_OUTPUTS = NUM_INPUTS + HL_SIZES[-1] + 1 # output layer takes in dimension of input + 1 + dimension of hl's\n",
    "\n",
    "net = Net(NUM_INPUTS,NUM_OUTPUTS,HL_SIZES)\n",
    "print(net)\n",
    "# print(list(net.named_parameters())) # or list(net.parameters()) if you don't need to view the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the loss function and the optimizer ###\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "L2_REG = 0.0\n",
    "MOMENTUM = 0.0\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=LEARNING_RATE,momentum=MOMENTUM,weight_decay=L2_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss = 0.0008020458044484258\n",
      "[2] loss = 0.0005283481441438198\n",
      "[4] loss = 0.00033751485170796514\n",
      "[6] loss = 0.0002417004870949313\n",
      "[8] loss = 0.0001948012795764953\n",
      "[10] loss = 0.00016521646466571838\n",
      "[12] loss = 0.00014263631601352245\n",
      "[14] loss = 0.00012405570305418223\n",
      "[16] loss = 0.00010835885041160509\n",
      "[18] loss = 9.497244172962382e-05\n",
      "[20] loss = 8.348090341314673e-05\n",
      "[22] loss = 7.354209810728207e-05\n",
      "[24] loss = 6.489935185527429e-05\n",
      "[26] loss = 5.736275124945678e-05\n",
      "[28] loss = 5.079899347038008e-05\n",
      "[30] loss = 4.5113032683730125e-05\n",
      "[32] loss = 4.0227077988674864e-05\n",
      "[34] loss = 3.6080276913708076e-05\n",
      "[36] loss = 3.260051744291559e-05\n",
      "[38] loss = 2.9712120522162877e-05\n",
      "[40] loss = 2.7337842766428366e-05\n",
      "[42] loss = 2.5391973395016976e-05\n",
      "[44] loss = 2.3801081624696963e-05\n",
      "[46] loss = 2.2496527890325524e-05\n",
      "[48] loss = 2.1420773919089697e-05\n",
      "[50] loss = 2.052466879831627e-05\n",
      "[52] loss = 1.9766708646784537e-05\n",
      "[54] loss = 1.9118360796710476e-05\n",
      "[56] loss = 1.8556647773948498e-05\n",
      "[58] loss = 1.8056936823995784e-05\n",
      "[60] loss = 1.7609050701139495e-05\n",
      "[62] loss = 1.7196151020471007e-05\n",
      "[64] loss = 1.681363573879935e-05\n",
      "[66] loss = 1.645298834773712e-05\n",
      "[68] loss = 1.611198422324378e-05\n",
      "[70] loss = 1.5782128684804775e-05\n",
      "[72] loss = 1.5466393961105496e-05\n",
      "[74] loss = 1.5158784663071856e-05\n",
      "[76] loss = 1.4862209354760125e-05\n",
      "[78] loss = 1.4572381587640848e-05\n",
      "[80] loss = 1.429330040991772e-05\n",
      "[82] loss = 1.402280577167403e-05\n",
      "[84] loss = 1.376152977172751e-05\n",
      "[86] loss = 1.3509647033060901e-05\n",
      "[88] loss = 1.3265734196465928e-05\n",
      "[90] loss = 1.3036278687650338e-05\n",
      "[92] loss = 1.2811511624022387e-05\n",
      "[94] loss = 1.259878627024591e-05\n",
      "[96] loss = 1.239703760802513e-05\n",
      "[98] loss = 1.2201389836263843e-05\n",
      "[100] loss = 1.2015604625048582e-05\n",
      "[102] loss = 1.1839203580166213e-05\n",
      "[104] loss = 1.1671479114738759e-05\n",
      "[106] loss = 1.1509770956763532e-05\n",
      "[108] loss = 1.135523689299589e-05\n",
      "[110] loss = 1.120657179853879e-05\n",
      "[112] loss = 1.1064626960433088e-05\n",
      "[114] loss = 1.0927386028924957e-05\n",
      "[116] loss = 1.0797253708005883e-05\n",
      "[118] loss = 1.0670203664631117e-05\n",
      "[120] loss = 1.0548592399572954e-05\n",
      "[122] loss = 1.0430186193843838e-05\n",
      "[124] loss = 1.0317319720343221e-05\n",
      "[126] loss = 1.0205864782619756e-05\n",
      "[128] loss = 1.0098683560499921e-05\n",
      "[130] loss = 9.993385901907459e-06\n",
      "[132] loss = 9.893844435282517e-06\n",
      "[134] loss = 9.792831406230107e-06\n",
      "[136] loss = 9.696127563074697e-06\n",
      "[138] loss = 9.604477781977039e-06\n",
      "[140] loss = 9.511494681646582e-06\n",
      "[142] loss = 9.420928108738735e-06\n",
      "[144] loss = 9.330097782367375e-06\n",
      "[146] loss = 9.24270716495812e-06\n",
      "[148] loss = 9.155959560303017e-06\n",
      "[150] loss = 9.07218418433331e-06\n",
      "[152] loss = 8.985449312604032e-06\n",
      "[154] loss = 8.902074114303105e-06\n",
      "[156] loss = 8.818465175863821e-06\n",
      "[158] loss = 8.735813025850803e-06\n",
      "[160] loss = 8.656204954604618e-06\n",
      "[162] loss = 8.574686034990009e-06\n",
      "[164] loss = 8.494617759424727e-06\n",
      "[166] loss = 8.418302968493663e-06\n",
      "[168] loss = 8.338473890034948e-06\n",
      "[170] loss = 8.26022369437851e-06\n",
      "[172] loss = 8.181640623661224e-06\n",
      "[174] loss = 8.105599590635393e-06\n",
      "[176] loss = 8.029877790249884e-06\n",
      "[178] loss = 7.951801308081485e-06\n",
      "[180] loss = 7.877131793065928e-06\n",
      "[182] loss = 7.800121238688007e-06\n",
      "[184] loss = 7.726152034592815e-06\n",
      "[186] loss = 7.651302439626306e-06\n",
      "[188] loss = 7.5777197707793675e-06\n",
      "[190] loss = 7.503555025323294e-06\n",
      "[192] loss = 7.430176992784254e-06\n",
      "[194] loss = 7.356759397225687e-06\n",
      "[196] loss = 7.2852353696362115e-06\n",
      "[198] loss = 7.2150901360146236e-06\n",
      "[200] loss = 7.143334642023547e-06\n",
      "[202] loss = 7.071977051964495e-06\n",
      "[204] loss = 7.003219252510462e-06\n",
      "[206] loss = 6.933905751793645e-06\n",
      "[208] loss = 6.863437647552928e-06\n",
      "[210] loss = 6.796597517677583e-06\n",
      "[212] loss = 6.728052994731115e-06\n",
      "[214] loss = 6.661800853180466e-06\n",
      "[216] loss = 6.594681053684326e-06\n",
      "[218] loss = 6.53091865387978e-06\n",
      "[220] loss = 6.4640971686458215e-06\n",
      "[222] loss = 6.402478447853355e-06\n",
      "[224] loss = 6.336576916510239e-06\n",
      "[226] loss = 6.274080533330562e-06\n",
      "[228] loss = 6.212773769220803e-06\n",
      "[230] loss = 6.153417416498996e-06\n",
      "[232] loss = 6.091071099945111e-06\n",
      "[234] loss = 6.030521035427228e-06\n",
      "[236] loss = 5.9718436205002945e-06\n",
      "[238] loss = 5.913233053433942e-06\n",
      "[240] loss = 5.856779353052843e-06\n",
      "[242] loss = 5.799352948088199e-06\n",
      "[244] loss = 5.743861493101576e-06\n",
      "[246] loss = 5.6898929869930726e-06\n",
      "[248] loss = 5.6334142755076755e-06\n",
      "[250] loss = 5.581126970355399e-06\n",
      "[252] loss = 5.53146992388065e-06\n",
      "[254] loss = 5.477474132931093e-06\n",
      "[256] loss = 5.425124527391745e-06\n",
      "[258] loss = 5.375175987865077e-06\n",
      "[260] loss = 5.323429832060356e-06\n",
      "[262] loss = 5.2752948249690235e-06\n",
      "[264] loss = 5.225965651334263e-06\n",
      "[266] loss = 5.179475010663737e-06\n",
      "[268] loss = 5.132360456627794e-06\n",
      "[270] loss = 5.0862158786912914e-06\n",
      "[272] loss = 5.041949862061301e-06\n",
      "[274] loss = 4.995962171960855e-06\n",
      "[276] loss = 4.952978088113014e-06\n",
      "[278] loss = 4.911063570034457e-06\n",
      "[280] loss = 4.866899416811066e-06\n",
      "[282] loss = 4.82628320241929e-06\n",
      "[284] loss = 4.784056272910675e-06\n",
      "[286] loss = 4.744239049614407e-06\n",
      "[288] loss = 4.704570528701879e-06\n",
      "[290] loss = 4.666433596867137e-06\n",
      "[292] loss = 4.627072485163808e-06\n",
      "[294] loss = 4.5915389819128904e-06\n",
      "[296] loss = 4.551060555968434e-06\n",
      "[298] loss = 4.515439741226146e-06\n",
      "[300] loss = 4.481837549974443e-06\n",
      "[302] loss = 4.446350430953316e-06\n",
      "[304] loss = 4.409217581269331e-06\n",
      "[306] loss = 4.378344328870298e-06\n",
      "[308] loss = 4.345638899394544e-06\n",
      "[310] loss = 4.312853889132384e-06\n",
      "[312] loss = 4.2800170376722235e-06\n",
      "[314] loss = 4.250104211678263e-06\n",
      "[316] loss = 4.218702088110149e-06\n",
      "[318] loss = 4.188206276012352e-06\n",
      "[320] loss = 4.159000127401669e-06\n",
      "[322] loss = 4.13055568060372e-06\n",
      "[324] loss = 4.102671937289415e-06\n",
      "[326] loss = 4.0737031667958945e-06\n",
      "[328] loss = 4.0460422496835236e-06\n",
      "[330] loss = 4.020419510197826e-06\n",
      "[332] loss = 3.994516646343982e-06\n",
      "[334] loss = 3.96847644879017e-06\n",
      "[336] loss = 3.943941010220442e-06\n",
      "[338] loss = 3.918042693840107e-06\n",
      "[340] loss = 3.8940088415984064e-06\n",
      "[342] loss = 3.869735337502789e-06\n",
      "[344] loss = 3.847153493552469e-06\n",
      "[346] loss = 3.824171471933369e-06\n",
      "[348] loss = 3.8021050841052784e-06\n",
      "[350] loss = 3.780119186558295e-06\n",
      "[352] loss = 3.7584634355880553e-06\n",
      "[354] loss = 3.7370914469647687e-06\n",
      "[356] loss = 3.7165539197303588e-06\n",
      "[358] loss = 3.696307430800516e-06\n",
      "[360] loss = 3.6760523016710067e-06\n",
      "[362] loss = 3.655821046777419e-06\n",
      "[364] loss = 3.6372814520291286e-06\n",
      "[366] loss = 3.617395123001188e-06\n",
      "[368] loss = 3.599831870815251e-06\n",
      "[370] loss = 3.5821617530018557e-06\n",
      "[372] loss = 3.5635514450405026e-06\n",
      "[374] loss = 3.5455382203508634e-06\n",
      "[376] loss = 3.5285884223412722e-06\n",
      "[378] loss = 3.5108755582768936e-06\n",
      "[380] loss = 3.495112650853116e-06\n",
      "[382] loss = 3.4785637126333313e-06\n",
      "[384] loss = 3.4632346341822995e-06\n",
      "[386] loss = 3.446268237894401e-06\n",
      "[388] loss = 3.430720653341268e-06\n",
      "[390] loss = 3.4160195809818106e-06\n",
      "[392] loss = 3.401640697120456e-06\n",
      "[394] loss = 3.3861572319437983e-06\n",
      "[396] loss = 3.3708029150147922e-06\n",
      "[398] loss = 3.357649120516726e-06\n",
      "[400] loss = 3.3433545922889607e-06\n",
      "[402] loss = 3.329730134282727e-06\n",
      "[404] loss = 3.31544765685976e-06\n",
      "[406] loss = 3.3030698887159815e-06\n",
      "[408] loss = 3.2882489904295653e-06\n",
      "[410] loss = 3.275559947724105e-06\n",
      "[412] loss = 3.2635368825140176e-06\n",
      "[414] loss = 3.25067753692565e-06\n",
      "[416] loss = 3.238780664105434e-06\n",
      "[418] loss = 3.225602995371446e-06\n",
      "[420] loss = 3.213522177247796e-06\n",
      "[422] loss = 3.2011014354793588e-06\n",
      "[424] loss = 3.1904967272566864e-06\n",
      "[426] loss = 3.177964117639931e-06\n",
      "[428] loss = 3.166430133205722e-06\n",
      "[430] loss = 3.1557790407532593e-06\n",
      "[432] loss = 3.1427941848960472e-06\n",
      "[434] loss = 3.1323672828875715e-06\n",
      "[436] loss = 3.1214615319186123e-06\n",
      "[438] loss = 3.110273155471077e-06\n",
      "[440] loss = 3.098874458373757e-06\n",
      "[442] loss = 3.0884302759659477e-06\n",
      "[444] loss = 3.076873326790519e-06\n",
      "[446] loss = 3.0671774311485933e-06\n",
      "[448] loss = 3.0569983664463507e-06\n",
      "[450] loss = 3.046753136004554e-06\n",
      "[452] loss = 3.03606020679581e-06\n",
      "[454] loss = 3.025910700671375e-06\n",
      "[456] loss = 3.0157470973790623e-06\n",
      "[458] loss = 3.0057833555474645e-06\n",
      "[460] loss = 2.9956045182188973e-06\n",
      "[462] loss = 2.9853695195924956e-06\n",
      "[464] loss = 2.9758170967397746e-06\n",
      "[466] loss = 2.965950898214942e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[468] loss = 2.9559198537754128e-06\n",
      "[470] loss = 2.94503274744784e-06\n",
      "[472] loss = 2.9362931854848284e-06\n",
      "[474] loss = 2.925607986981049e-06\n",
      "[476] loss = 2.9160351004975382e-06\n",
      "[478] loss = 2.9071861717966385e-06\n",
      "[480] loss = 2.895805209846003e-06\n",
      "[482] loss = 2.8863207717222394e-06\n",
      "[484] loss = 2.8771794404747197e-06\n",
      "[486] loss = 2.86688646156108e-06\n",
      "[488] loss = 2.856801984307822e-06\n",
      "[490] loss = 2.8469733024394372e-06\n",
      "[492] loss = 2.8375616238918155e-06\n",
      "[494] loss = 2.827337993949186e-06\n",
      "[496] loss = 2.8174754334031604e-06\n",
      "[498] loss = 2.8084145924367476e-06\n",
      "[500] loss = 2.797740080495714e-06\n",
      "[502] loss = 2.7872038117493503e-06\n",
      "[504] loss = 2.7773498914029915e-06\n",
      "[506] loss = 2.7676096578943543e-06\n",
      "[508] loss = 2.757145011855755e-06\n",
      "[510] loss = 2.7468095140648074e-06\n",
      "[512] loss = 2.7368851078790613e-06\n",
      "[514] loss = 2.725084641497233e-06\n",
      "[516] loss = 2.715176833589794e-06\n",
      "[518] loss = 2.704494818317471e-06\n",
      "[520] loss = 2.694902377697872e-06\n",
      "[522] loss = 2.6840016289497726e-06\n",
      "[524] loss = 2.6725870156951714e-06\n",
      "[526] loss = 2.6631819309841376e-06\n",
      "[528] loss = 2.6509906092542224e-06\n",
      "[530] loss = 2.63959850599349e-06\n",
      "[532] loss = 2.6289746983820805e-06\n",
      "[534] loss = 2.6172842808591668e-06\n",
      "[536] loss = 2.607642272778321e-06\n",
      "[538] loss = 2.595448904685327e-06\n",
      "[540] loss = 2.5838983219728107e-06\n",
      "[542] loss = 2.573384790593991e-06\n",
      "[544] loss = 2.5610825105104595e-06\n",
      "[546] loss = 2.549763166825869e-06\n",
      "[548] loss = 2.538734406698495e-06\n",
      "[550] loss = 2.527329570511938e-06\n",
      "[552] loss = 2.5143538096017437e-06\n",
      "[554] loss = 2.5032074972841656e-06\n",
      "[556] loss = 2.4905721147661097e-06\n",
      "[558] loss = 2.4787250367808156e-06\n",
      "[560] loss = 2.4653868422319647e-06\n",
      "[562] loss = 2.4538048819522373e-06\n",
      "[564] loss = 2.441451442791731e-06\n",
      "[566] loss = 2.42906480707461e-06\n",
      "[568] loss = 2.4156206563930027e-06\n",
      "[570] loss = 2.4029895939747803e-06\n",
      "[572] loss = 2.3897105165815447e-06\n",
      "[574] loss = 2.3766820049786475e-06\n",
      "[576] loss = 2.3640420749870827e-06\n",
      "[578] loss = 2.3516192868555663e-06\n",
      "[580] loss = 2.3376815079245716e-06\n",
      "[582] loss = 2.324526121810777e-06\n",
      "[584] loss = 2.3112736471375683e-06\n",
      "[586] loss = 2.297699211339932e-06\n",
      "[588] loss = 2.284099309690646e-06\n",
      "[590] loss = 2.2705562514602207e-06\n",
      "[592] loss = 2.2564536266145296e-06\n",
      "[594] loss = 2.241746869913186e-06\n",
      "[596] loss = 2.2285159957391443e-06\n",
      "[598] loss = 2.2145059119793586e-06\n",
      "[600] loss = 2.1993321297486546e-06\n",
      "[602] loss = 2.185290213674307e-06\n",
      "[604] loss = 2.1724956695834408e-06\n",
      "[606] loss = 2.1575895061687334e-06\n",
      "[608] loss = 2.1431469576782547e-06\n",
      "[610] loss = 2.1287385152390925e-06\n",
      "[612] loss = 2.1142404875718057e-06\n",
      "[614] loss = 2.0993688849557657e-06\n",
      "[616] loss = 2.084486141029629e-06\n",
      "[618] loss = 2.069692527584266e-06\n",
      "[620] loss = 2.0540865079965442e-06\n",
      "[622] loss = 2.0408474483701866e-06\n",
      "[624] loss = 2.0244988263584673e-06\n",
      "[626] loss = 2.0101397240068763e-06\n",
      "[628] loss = 1.995527554754517e-06\n",
      "[630] loss = 1.980195293072029e-06\n",
      "[632] loss = 1.965667479453259e-06\n",
      "[634] loss = 1.9493791114655323e-06\n",
      "[636] loss = 1.935058207891416e-06\n",
      "[638] loss = 1.919831674968009e-06\n",
      "[640] loss = 1.905066596918914e-06\n",
      "[642] loss = 1.8895092352977372e-06\n",
      "[644] loss = 1.8736880065262085e-06\n",
      "[646] loss = 1.8588852981338277e-06\n",
      "[648] loss = 1.8436140862831962e-06\n",
      "[650] loss = 1.8284928273715195e-06\n",
      "[652] loss = 1.8129178442904959e-06\n",
      "[654] loss = 1.7971464103538892e-06\n",
      "[656] loss = 1.7817209254644695e-06\n",
      "[658] loss = 1.7674369701126125e-06\n",
      "[660] loss = 1.7526997453387594e-06\n",
      "[662] loss = 1.7371411331623676e-06\n",
      "[664] loss = 1.7220548897967092e-06\n",
      "[666] loss = 1.7072800346795702e-06\n",
      "[668] loss = 1.6906180917430902e-06\n",
      "[670] loss = 1.6768735804362223e-06\n",
      "[672] loss = 1.6621277154627023e-06\n",
      "[674] loss = 1.647232920731767e-06\n",
      "[676] loss = 1.6327564935636474e-06\n",
      "[678] loss = 1.6180420061573386e-06\n",
      "[680] loss = 1.602750785423268e-06\n",
      "[682] loss = 1.588630198057217e-06\n",
      "[684] loss = 1.573923213982198e-06\n",
      "[686] loss = 1.559440306664328e-06\n",
      "[688] loss = 1.5447604937435244e-06\n",
      "[690] loss = 1.5309263972085319e-06\n",
      "[692] loss = 1.51597305375617e-06\n",
      "[694] loss = 1.5018177919046138e-06\n",
      "[696] loss = 1.4877080047881464e-06\n",
      "[698] loss = 1.4734906699231942e-06\n",
      "[700] loss = 1.4591817034670385e-06\n",
      "[702] loss = 1.4451391052716644e-06\n",
      "[704] loss = 1.4317697605292778e-06\n",
      "[706] loss = 1.4174752323015127e-06\n",
      "[708] loss = 1.4047403738004505e-06\n",
      "[710] loss = 1.3901803868066054e-06\n",
      "[712] loss = 1.3778536640529637e-06\n",
      "[714] loss = 1.363582214253256e-06\n",
      "[716] loss = 1.3496055544237606e-06\n",
      "[718] loss = 1.3374778973229695e-06\n",
      "[720] loss = 1.3235844562586863e-06\n",
      "[722] loss = 1.311110167989682e-06\n",
      "[724] loss = 1.2982313819520641e-06\n",
      "[726] loss = 1.2855966815550346e-06\n",
      "[728] loss = 1.272485405934276e-06\n",
      "[730] loss = 1.2598659395735012e-06\n",
      "[732] loss = 1.2474808954721084e-06\n",
      "[734] loss = 1.234838009622763e-06\n",
      "[736] loss = 1.2231580512889195e-06\n",
      "[738] loss = 1.2105239193260786e-06\n",
      "[740] loss = 1.1986555819021305e-06\n",
      "[742] loss = 1.1866936802107375e-06\n",
      "[744] loss = 1.1746977861548658e-06\n",
      "[746] loss = 1.1639328931778437e-06\n",
      "[748] loss = 1.1515837741171708e-06\n",
      "[750] loss = 1.1409355238356511e-06\n",
      "[752] loss = 1.1286658718745457e-06\n",
      "[754] loss = 1.1181717809449765e-06\n",
      "[756] loss = 1.1065999387938064e-06\n",
      "[758] loss = 1.0958578968711663e-06\n",
      "[760] loss = 1.0857024790311698e-06\n",
      "[762] loss = 1.0739321396613377e-06\n",
      "[764] loss = 1.0632558087309008e-06\n",
      "[766] loss = 1.0534287184782443e-06\n",
      "[768] loss = 1.0431655255160877e-06\n",
      "[770] loss = 1.032383693200245e-06\n",
      "[772] loss = 1.0226013955616509e-06\n",
      "[774] loss = 1.0124323353011278e-06\n",
      "[776] loss = 1.0029646091425093e-06\n",
      "[778] loss = 9.926582151820185e-07\n",
      "[780] loss = 9.830747558225994e-07\n",
      "[782] loss = 9.734453669807408e-07\n",
      "[784] loss = 9.637619768909644e-07\n",
      "[786] loss = 9.549081596560427e-07\n",
      "[788] loss = 9.457407941226847e-07\n",
      "[790] loss = 9.361465345136821e-07\n",
      "[792] loss = 9.276369041799626e-07\n",
      "[794] loss = 9.190708851747331e-07\n",
      "[796] loss = 9.101325417759654e-07\n",
      "[798] loss = 9.011723136609362e-07\n",
      "[800] loss = 8.929252430789347e-07\n",
      "[802] loss = 8.84269468315324e-07\n",
      "[804] loss = 8.753852966947306e-07\n",
      "[806] loss = 8.687041486155067e-07\n",
      "[808] loss = 8.607691484030511e-07\n",
      "[810] loss = 8.52005314300186e-07\n",
      "[812] loss = 8.442801231467456e-07\n",
      "[814] loss = 8.367852046831104e-07\n",
      "[816] loss = 8.289343327305687e-07\n",
      "[818] loss = 8.205137191907852e-07\n",
      "[820] loss = 8.132742550515104e-07\n",
      "[822] loss = 8.064109806582564e-07\n",
      "[824] loss = 7.995427608875616e-07\n",
      "[826] loss = 7.924840019768453e-07\n",
      "[828] loss = 7.851509167267068e-07\n",
      "[830] loss = 7.785653224345879e-07\n",
      "[832] loss = 7.713494483141403e-07\n",
      "[834] loss = 7.65210359077173e-07\n",
      "[836] loss = 7.583822707601939e-07\n",
      "[838] loss = 7.526036824856419e-07\n",
      "[840] loss = 7.453409693880531e-07\n",
      "[842] loss = 7.391311669380229e-07\n",
      "[844] loss = 7.329277877943241e-07\n",
      "[846] loss = 7.26680013940495e-07\n",
      "[848] loss = 7.205886163319519e-07\n",
      "[850] loss = 7.160807058426144e-07\n",
      "[852] loss = 7.097359002727899e-07\n",
      "[854] loss = 7.035299063318234e-07\n",
      "[856] loss = 6.972779260649986e-07\n",
      "[858] loss = 6.919119641679572e-07\n",
      "[860] loss = 6.865609520900762e-07\n",
      "[862] loss = 6.806200190112577e-07\n",
      "[864] loss = 6.757213100172521e-07\n",
      "[866] loss = 6.704125894430035e-07\n",
      "[868] loss = 6.657426183664938e-07\n",
      "[870] loss = 6.605445719287673e-07\n",
      "[872] loss = 6.546194981638109e-07\n",
      "[874] loss = 6.505297278636135e-07\n",
      "[876] loss = 6.455597372223565e-07\n",
      "[878] loss = 6.403259362741665e-07\n",
      "[880] loss = 6.359852591231174e-07\n",
      "[882] loss = 6.310912681328773e-07\n",
      "[884] loss = 6.271160373216844e-07\n",
      "[886] loss = 6.218788257683627e-07\n",
      "[888] loss = 6.174045665829908e-07\n",
      "[890] loss = 6.135082344371767e-07\n",
      "[892] loss = 6.088756094868586e-07\n",
      "[894] loss = 6.04758554345608e-07\n",
      "[896] loss = 6.015761755406857e-07\n",
      "[898] loss = 5.976443731015024e-07\n",
      "[900] loss = 5.930669431108981e-07\n",
      "[902] loss = 5.89541457429732e-07\n",
      "[904] loss = 5.849709054928098e-07\n",
      "[906] loss = 5.81251697440166e-07\n",
      "[908] loss = 5.773551379206765e-07\n",
      "[910] loss = 5.738298227697669e-07\n",
      "[912] loss = 5.704821433027973e-07\n",
      "[914] loss = 5.663402475875046e-07\n",
      "[916] loss = 5.635517368318688e-07\n",
      "[918] loss = 5.592642651208735e-07\n",
      "[920] loss = 5.564460821005923e-07\n",
      "[922] loss = 5.532577915801085e-07\n",
      "[924] loss = 5.49200706245756e-07\n",
      "[926] loss = 5.467904884426389e-07\n",
      "[928] loss = 5.432909233604732e-07\n",
      "[930] loss = 5.398411531132297e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[932] loss = 5.374929514800897e-07\n",
      "[934] loss = 5.340427833289141e-07\n",
      "[936] loss = 5.304186743160244e-07\n",
      "[938] loss = 5.280531922835507e-07\n",
      "[940] loss = 5.249842729426746e-07\n",
      "[942] loss = 5.21909385042818e-07\n",
      "[944] loss = 5.19136506227369e-07\n",
      "[946] loss = 5.165431389286823e-07\n",
      "[948] loss = 5.147129513716209e-07\n",
      "[950] loss = 5.106148250888509e-07\n",
      "[952] loss = 5.078716185380472e-07\n",
      "[954] loss = 5.052534106653184e-07\n",
      "[956] loss = 5.029937142353447e-07\n",
      "[958] loss = 5.0015125907521e-07\n",
      "[960] loss = 4.980112180419383e-07\n",
      "[962] loss = 4.948007585880987e-07\n",
      "[964] loss = 4.930153636450996e-07\n",
      "[966] loss = 4.900462045043241e-07\n",
      "[968] loss = 4.877767878497252e-07\n",
      "[970] loss = 4.858923716710706e-07\n",
      "[972] loss = 4.837250457967457e-07\n",
      "[974] loss = 4.809776896763651e-07\n",
      "[976] loss = 4.796187340616598e-07\n",
      "[978] loss = 4.7658880930612213e-07\n",
      "[980] loss = 4.7455284857278457e-07\n",
      "[982] loss = 4.7273726977437036e-07\n",
      "[984] loss = 4.709147276571457e-07\n",
      "[986] loss = 4.680333063333819e-07\n",
      "[988] loss = 4.6646195528410317e-07\n",
      "[990] loss = 4.638422979041934e-07\n",
      "[992] loss = 4.616549631464295e-07\n",
      "[994] loss = 4.600194358772569e-07\n",
      "[996] loss = 4.5858783437324746e-07\n",
      "[998] loss = 4.566919074022735e-07\n",
      "[1000] loss = 4.5507542267841927e-07\n",
      "[1002] loss = 4.529960619947815e-07\n",
      "[1004] loss = 4.512324665029155e-07\n",
      "[1006] loss = 4.493928145166137e-07\n",
      "[1008] loss = 4.4707192614623636e-07\n",
      "[1010] loss = 4.4524969666781544e-07\n",
      "[1012] loss = 4.438612108970119e-07\n",
      "[1014] loss = 4.426265149959363e-07\n",
      "[1016] loss = 4.411110978708166e-07\n",
      "[1018] loss = 4.3896034185308963e-07\n",
      "[1020] loss = 4.3772686808551953e-07\n",
      "[1022] loss = 4.356166414254403e-07\n",
      "[1024] loss = 4.347926960690529e-07\n",
      "[1026] loss = 4.3280545014567906e-07\n",
      "[1028] loss = 4.3129207938363834e-07\n",
      "[1030] loss = 4.2949380940626725e-07\n",
      "[1032] loss = 4.281049541532411e-07\n",
      "[1034] loss = 4.265052098162414e-07\n",
      "[1036] loss = 4.2533935129540623e-07\n",
      "[1038] loss = 4.242677675847517e-07\n",
      "[1040] loss = 4.22632894014896e-07\n",
      "[1042] loss = 4.205709842608485e-07\n",
      "[1044] loss = 4.199909540147928e-07\n",
      "[1046] loss = 4.1839641085061885e-07\n",
      "[1048] loss = 4.170250349488924e-07\n",
      "[1050] loss = 4.1529756344971247e-07\n",
      "[1052] loss = 4.1439977849222487e-07\n",
      "[1054] loss = 4.1315553289678064e-07\n",
      "[1056] loss = 4.116008085475187e-07\n",
      "[1058] loss = 4.106549909010937e-07\n",
      "[1060] loss = 4.0911132259680016e-07\n",
      "[1062] loss = 4.078708855104196e-07\n",
      "[1064] loss = 4.0708829374125344e-07\n",
      "[1066] loss = 4.0576423998572864e-07\n",
      "[1068] loss = 4.050049824400048e-07\n",
      "[1070] loss = 4.0332815842702985e-07\n",
      "[1072] loss = 4.02767398099968e-07\n",
      "[1074] loss = 4.0138104395737173e-07\n",
      "[1076] loss = 4.0027808267950604e-07\n",
      "[1078] loss = 3.992688277776324e-07\n",
      "[1080] loss = 3.9764765347172215e-07\n",
      "[1082] loss = 3.9674677054790664e-07\n",
      "[1084] loss = 3.959406456033321e-07\n",
      "[1086] loss = 3.9514677041552204e-07\n",
      "[1088] loss = 3.941765100989869e-07\n",
      "[1090] loss = 3.9337234625236306e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0c4c1684afcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training the network ###\n",
    "print_less_often = 2\n",
    "eps = 1e-13\n",
    "train_loss = []\n",
    "maxEpochs = 20000\n",
    "prev_loss = 0\n",
    "curr_loss = 1e10\n",
    "epoch = 0\n",
    "net.train()\n",
    "\n",
    "\n",
    "while (epoch <= maxEpochs): \n",
    "\n",
    "    if epoch % print_less_often == 0:\n",
    "        if np.abs(prev_loss - curr_loss) < eps:\n",
    "            break\n",
    "        prev_loss = curr_loss\n",
    "\n",
    "    for i in range(0,trainXp.shape[0]):\n",
    "        \n",
    "        dt = dt_list[i]\n",
    "        \n",
    "        eL = torch.diag_embed(torch.exp(net.L*dt)) # exponential of the eigs, then embedded into a diagonal matrix\n",
    "        KG = torch.matmul(torch.matmul(net.V,eL),torch.pinverse(net.V)) # matrix representation of Koopman generator\n",
    "        \n",
    "        Kpsixp = torch.matmul(net(trainXp[i:i+1]),KG) \n",
    "        psixf = net(trainXf[i:i+1])\n",
    "        loss = loss_func(psixf, Kpsixp)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    curr_loss = loss.item()\n",
    "\n",
    "    if epoch % print_less_often == 0:\n",
    "        print('['+str(epoch)+']'+' loss = '+str(loss.item()))\n",
    "        train_loss.append(loss.item()) \n",
    "    epoch+=1\n",
    "\n",
    "train_loss.append(loss.item())\n",
    "print('['+str(epoch)+']'+' loss = '+ str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':14});\n",
    "plt.semilogy(train_loss,lw=4);\n",
    "plt.ylabel('MSE Loss');\n",
    "plt.xlabel('Epoch');\n",
    "# plt.savefig('Loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([NUM_INPUTS,NUM_OUTPUTS,HL_SIZES],open(trained_models_path+'/KG_toggleSwitch_netsize.pickle','wb'))\n",
    "torch.save(net.state_dict(), trained_models_path+'/KG_toggleSwitch_net.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
